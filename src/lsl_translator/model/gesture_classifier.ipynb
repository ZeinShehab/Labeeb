{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "igMyGnjE9hEp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\zeins\\anaconda3\\envs\\lsl\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2HDvhIu9hEr"
      },
      "source": [
        "# Specify each path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9NvZP2Zn9hEy"
      },
      "outputs": [],
      "source": [
        "data_directory = \"../../../data/\"\n",
        "train_dataset = f'{data_directory}augmented_gestures_train.csv'\n",
        "test_dataset = f'{data_directory}augmented_gestures_test.csv'\n",
        "model_save_path = './gesture_classifier.keras'\n",
        "tflite_save_path = './gesture_classifier.tflite'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5oMH7x19hEz"
      },
      "source": [
        "# Set number of classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "du4kodXL9hEz"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42\n",
        "NUM_CLASSES = 5\n",
        "NUM_SEQUENCE_FRAMES = 10\n",
        "MULTI_HAND_LANDMARKS = 126\n",
        "FEATURE_DIM = MULTI_HAND_LANDMARKS * NUM_SEQUENCE_FRAMES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjnL0uso9hEz"
      },
      "source": [
        "# Dataset reading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QT5ZqtEz9hE0"
      },
      "outputs": [],
      "source": [
        "X_train = np.loadtxt(train_dataset, delimiter=',', dtype='float32', usecols=list(range(1, FEATURE_DIM + 1)))\n",
        "X_test_data = np.loadtxt(test_dataset, delimiter=',', dtype='float32', usecols=list(range(1, FEATURE_DIM + 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QmoKFsp49hE0"
      },
      "outputs": [],
      "source": [
        "y_train = np.loadtxt(train_dataset, delimiter=',', dtype='int32', usecols=(0))\n",
        "y_test_data = np.loadtxt(test_dataset, delimiter=',', dtype='int32', usecols=(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test, X_valid, y_test, y_valid = train_test_split(X_test_data, y_test_data, train_size=0.75, random_state=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4]\n",
            "[0 1 2 3 4]\n",
            "[0 1 2 3 4]\n"
          ]
        }
      ],
      "source": [
        "print(np.unique(y_train))\n",
        "print(np.unique(y_test))\n",
        "print(np.unique(y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(551, 1260)\n",
            "(551,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxK_lETT9hE0"
      },
      "source": [
        "# Model building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vHBmUf1t9hE1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\zeins\\anaconda3\\envs\\lsl\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input((FEATURE_DIM, )),\n",
        "    # tf.keras.layers.BatchNormalization(),  # Normalizes input features\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),  # Dropout for regularization\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),  # Dropout for regularization\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),  # Dropout for regularization\n",
        "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')  # Output layer with 5 units\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    model_save_path, verbose=1, save_weights_only=False)\n",
        "# Callback for early stopping\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\zeins\\anaconda3\\envs\\lsl\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Model compilation\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "WARNING:tensorflow:From c:\\Users\\zeins\\anaconda3\\envs\\lsl\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\zeins\\anaconda3\\envs\\lsl\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "1/5 [=====>........................] - ETA: 3s - loss: 5.5453 - accuracy: 0.1641\n",
            "Epoch 1: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 1s 66ms/step - loss: 2.7275 - accuracy: 0.4555 - val_loss: 0.6910 - val_accuracy: 0.6145\n",
            "Epoch 2/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.8273 - accuracy: 0.7578\n",
            "Epoch 2: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.6586 - accuracy: 0.8185 - val_loss: 0.5977 - val_accuracy: 0.6145\n",
            "Epoch 3/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.4664 - accuracy: 0.8359\n",
            "Epoch 3: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.3881 - accuracy: 0.8748 - val_loss: 0.4571 - val_accuracy: 0.8193\n",
            "Epoch 4/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.2979 - accuracy: 0.8984\n",
            "Epoch 4: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.2112 - accuracy: 0.9383 - val_loss: 0.4086 - val_accuracy: 0.8193\n",
            "Epoch 5/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.1134 - accuracy: 0.9609\n",
            "Epoch 5: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.1280 - accuracy: 0.9619 - val_loss: 0.2238 - val_accuracy: 0.8193\n",
            "Epoch 6/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.2327 - accuracy: 0.9453\n",
            "Epoch 6: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.1546 - accuracy: 0.9637 - val_loss: 0.1677 - val_accuracy: 0.9880\n",
            "Epoch 7/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.2699 - accuracy: 0.9531\n",
            "Epoch 7: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.1425 - accuracy: 0.9637 - val_loss: 0.1508 - val_accuracy: 0.9880\n",
            "Epoch 8/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0689 - accuracy: 0.9688\n",
            "Epoch 8: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0882 - accuracy: 0.9764 - val_loss: 0.1730 - val_accuracy: 0.9880\n",
            "Epoch 9/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0613 - accuracy: 0.9766\n",
            "Epoch 9: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0692 - accuracy: 0.9764 - val_loss: 0.1714 - val_accuracy: 0.8193\n",
            "Epoch 10/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0789 - accuracy: 0.9844\n",
            "Epoch 10: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0502 - accuracy: 0.9891 - val_loss: 0.5375 - val_accuracy: 0.8313\n",
            "Epoch 11/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0787 - accuracy: 0.9844\n",
            "Epoch 11: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0807 - accuracy: 0.9746 - val_loss: 0.6057 - val_accuracy: 0.8193\n",
            "Epoch 12/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0391 - accuracy: 0.9766\n",
            "Epoch 12: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0294 - accuracy: 0.9891 - val_loss: 0.4122 - val_accuracy: 0.8193\n",
            "Epoch 13/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0553 - accuracy: 0.9922\n",
            "Epoch 13: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0676 - accuracy: 0.9819 - val_loss: 0.1857 - val_accuracy: 0.8313\n",
            "Epoch 14/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 14: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0278 - accuracy: 0.9927 - val_loss: 0.1260 - val_accuracy: 1.0000\n",
            "Epoch 15/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.1125 - accuracy: 0.9844\n",
            "Epoch 15: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0840 - accuracy: 0.9873 - val_loss: 0.0478 - val_accuracy: 1.0000\n",
            "Epoch 16/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0395 - accuracy: 0.9922\n",
            "Epoch 16: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0513 - accuracy: 0.9855 - val_loss: 0.0810 - val_accuracy: 0.9880\n",
            "Epoch 17/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 17: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 23ms/step - loss: 0.0083 - accuracy: 0.9982 - val_loss: 0.3656 - val_accuracy: 0.8193\n",
            "Epoch 18/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 18: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0337 - accuracy: 0.9909 - val_loss: 0.4496 - val_accuracy: 0.8193\n",
            "Epoch 19/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0294 - accuracy: 0.9922\n",
            "Epoch 19: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0246 - accuracy: 0.9909 - val_loss: 0.3859 - val_accuracy: 0.8193\n",
            "Epoch 20/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0472 - accuracy: 0.9844\n",
            "Epoch 20: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0213 - accuracy: 0.9909 - val_loss: 0.2139 - val_accuracy: 0.8313\n",
            "Epoch 21/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 21: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0420 - accuracy: 0.9946 - val_loss: 0.5505 - val_accuracy: 0.8795\n",
            "Epoch 22/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0158 - accuracy: 0.9922\n",
            "Epoch 22: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0359 - accuracy: 0.9927 - val_loss: 0.6355 - val_accuracy: 0.8795\n",
            "Epoch 23/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 23: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0083 - accuracy: 0.9964 - val_loss: 0.3186 - val_accuracy: 0.8795\n",
            "Epoch 24/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0336 - accuracy: 0.9922\n",
            "Epoch 24: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0212 - accuracy: 0.9946 - val_loss: 0.1052 - val_accuracy: 1.0000\n",
            "Epoch 25/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 25: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.2090 - val_accuracy: 0.8193\n",
            "Epoch 26/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0288 - accuracy: 0.9922\n",
            "Epoch 26: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.0460 - accuracy: 0.9891 - val_loss: 0.1315 - val_accuracy: 0.9880\n",
            "Epoch 27/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 27: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0196 - accuracy: 0.9946 - val_loss: 0.4128 - val_accuracy: 0.8313\n",
            "Epoch 28/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0114 - accuracy: 0.9922\n",
            "Epoch 28: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0229 - accuracy: 0.9909 - val_loss: 0.2569 - val_accuracy: 0.8313\n",
            "Epoch 29/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 29: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2257 - val_accuracy: 0.8795\n",
            "Epoch 30/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0600 - accuracy: 0.9922\n",
            "Epoch 30: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 23ms/step - loss: 0.0287 - accuracy: 0.9891 - val_loss: 0.3639 - val_accuracy: 0.8795\n",
            "Epoch 31/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0175 - accuracy: 0.9922\n",
            "Epoch 31: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0308 - accuracy: 0.9909 - val_loss: 1.2549 - val_accuracy: 0.7108\n",
            "Epoch 32/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0713 - accuracy: 0.9844\n",
            "Epoch 32: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0350 - accuracy: 0.9891 - val_loss: 0.6976 - val_accuracy: 0.8313\n",
            "Epoch 33/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.1002 - accuracy: 0.9766\n",
            "Epoch 33: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0420 - accuracy: 0.9891 - val_loss: 0.4561 - val_accuracy: 0.8193\n",
            "Epoch 34/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 34: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0392 - accuracy: 0.9927 - val_loss: 1.0243 - val_accuracy: 0.8193\n",
            "Epoch 35/1000\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0168 - accuracy: 0.9922\n",
            "Epoch 35: saving model to .\\gesture_classifier.keras\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0135 - accuracy: 0.9946 - val_loss: 0.1297 - val_accuracy: 1.0000\n",
            "Epoch 35: early stopping\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x20e13054d60>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=1000,\n",
        "    batch_size=128,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    callbacks=[cp_callback, es_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 4ms/step - loss: 0.3980 - accuracy: 0.9717\n"
          ]
        }
      ],
      "source": [
        "val_loss, val_acc = model.evaluate(X_test, y_test, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WirBl-JE9hE3",
        "outputId": "71b30ca2-8294-4d9d-8aa2-800d90d399de",
        "scrolled": true,
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 85ms/step\n",
            "[5.6423342e-01 1.8590833e-03 3.2266413e-05 4.5423285e-04 4.3342093e-01]\n",
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# Inference test\n",
        "predict_result = model.predict(np.array([X_test[0]]))\n",
        "print(np.squeeze(predict_result))\n",
        "print(np.argmax(np.squeeze(predict_result)))\n",
        "print(y_test[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3U4yNWx9hE4"
      },
      "source": [
        "# Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "AP1V6SCk9hE5",
        "outputId": "08e41a80-7a4a-4619-8125-ecc371368d19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 700x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96        52\n",
            "           1       1.00      1.00      1.00        43\n",
            "           2       0.96      0.98      0.97        47\n",
            "           3       0.98      0.96      0.97        54\n",
            "           4       0.94      0.98      0.96        51\n",
            "\n",
            "    accuracy                           0.97       247\n",
            "   macro avg       0.97      0.97      0.97       247\n",
            "weighted avg       0.97      0.97      0.97       247\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def print_confusion_matrix(y_true, y_pred, report=True):\n",
        "    labels = sorted(list(set(y_true)))\n",
        "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    \n",
        "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
        " \n",
        "    fig, ax = plt.subplots(figsize=(7, 6))\n",
        "    sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
        "    ax.set_ylim(len(set(y_true)), 0)\n",
        "    plt.show()\n",
        "    \n",
        "    if report:\n",
        "        print('Classification Report')\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "Y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "print_confusion_matrix(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNP6aqzc9hE5"
      },
      "source": [
        "# Convert to model for Tensorflow-Lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ODjnYyld9hE6"
      },
      "outputs": [],
      "source": [
        "# Save as a model dedicated to inference\n",
        "model.save(model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\zeins\\AppData\\Local\\Temp\\tmpwcrwrlsy\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\zeins\\AppData\\Local\\Temp\\tmpwcrwrlsy\\assets\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "369272"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quantized_model = converter.convert()\n",
        "\n",
        "open(tflite_save_path, 'wb').write(tflite_quantized_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=tflite_save_path)\n",
        "interpreter.allocate_tensors()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get I / O tensor\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1. 0. 0. 0. 0.]\n",
            "0\n",
            "CPU times: total: 0 ns\n",
            "Wall time: 999 Âµs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Inference implementation\n",
        "interpreter.invoke()\n",
        "tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(np.squeeze(tflite_results))\n",
        "print(np.argmax(np.squeeze(tflite_results)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "keypoint_classification_EN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "85b733480cc67b8cf4f7b44ea859236688838663f0d4c864dc5b1de551a5e467"
    },
    "kernelspec": {
      "display_name": "Python 3.9.4 64-bit ('mpenv': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
